{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4565623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#from matplotlib import transforms\n",
    "from geopy.geocoders import Nominatim\n",
    "from networks import *\n",
    "from dataloader import *\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image as im\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "#from utilities.video_transforms import *\n",
    "#from utilities.volume_transforms import  *\n",
    "\n",
    "import torchvision.transforms as transforms \n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "from einops import rearrange\n",
    "import csv\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists \n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoFeatureExtractor, ViTMAEModel\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f06ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = config.getopt()%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d476944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing\n",
    "#import argparge\n",
    "import torch\n",
    "import networks\n",
    "\n",
    "def getopt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    opt = parser.parse_args(\"\")\n",
    "    opt.kernels = multiprocessing.cpu_count()\n",
    "\n",
    "    opt.BDDfolder = \"/home/alec/Documents/BigDatasets/BDD100k_Big/Ground/\"\n",
    "    opt.yfcc25600folder = \"/home/alec/Documents/BigDatasets/yfcc25600/\"\n",
    "    opt.mp16folder = \"/home/alec/Documents/BigDatasets/mp16/\"\n",
    "    opt.im2gps3k = \"/home/alec/Documents/SmallDatasets/im2gps3ktest/\"\n",
    "\n",
    "    opt.resources = \"/home/alec/Documents/BigDatasets/resources/\"\n",
    "\n",
    "    opt.size = 224\n",
    "\n",
    "    opt.n_epochs = 20\n",
    "\n",
    "    #opt.description = 'GeoGuess4-4.2M-Im2GPS3k-F*'\n",
    "    opt.description = 'Testing IsoMax fine'\n",
    "    opt.evaluate = False\n",
    "\n",
    "    # How often to report loss\n",
    "    opt.loss_per_epoch = 100\n",
    "\n",
    "    # How often to validate\n",
    "    opt.val_per_epoch = 25\n",
    "\n",
    "    opt.lr = 0.1\n",
    "    opt.step_size = 3\n",
    "    opt.hier_eval = True\n",
    "    opt.scene = False\n",
    "    \n",
    "    opt.loss = 'ce'\n",
    "    opt.model = 'GeoGuess1'\n",
    "    opt.archname = opt.model\n",
    "\n",
    "    opt.wandb = True\n",
    "\n",
    "    opt.batch_size = 200\n",
    "    opt.distances = [2500, 750, 200, 25, 1]\n",
    "    opt.trainset = 'train'\n",
    "    opt.testset1 = 'im2gps3k'\n",
    "    opt.testset2 = 'yfcc25600'\n",
    "    opt.device = torch.device('cuda')\n",
    "\n",
    "\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4260d232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810bfb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = getopt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c082e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pickle.load(open(\"weights/datasettemp.pkl\", \"rb\"))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=opt.batch_size, num_workers=opt.kernels, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf16f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTMAEModel: ['decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.mask_token', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.3.layernorm_after.bias']\n",
      "- This IS expected if you are initializing ViTMAEModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTMAEModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb739088",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(opt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75556d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                          | 0/23270 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63662071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████████▌                                                                             | 3905/23270 [17:13<1:25:25,  3.78it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m embeds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i ,(imgs, classes, scenes, gps) \u001b[38;5;129;01min\u001b[39;00m bar:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#coarse_classes = classes[:,0].to(opt.device)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#medium_classes = classes[:,1]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(opt\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/geoloc/lib/python3.9/site-packages/tqdm/std.py:1180\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1177\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geoloc/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/geoloc/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1195\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1195\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geoloc/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1161\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1161\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1163\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/geoloc/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1026\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mEMFILE:\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1027\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many open files. Communication with the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1028\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m workers is no longer possible. Please increase the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m limit using `ulimit -n` in the shell or change the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sharing strategy by calling\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch.multiprocessing.set_sharing_strategy(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_system\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1032\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m at the beginning of your code\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code"
     ]
    }
   ],
   "source": [
    "embeds = []\n",
    "labels = []\n",
    "for i ,(imgs, classes, scenes, gps) in bar:\n",
    "    #coarse_classes = classes[:,0].to(opt.device)\n",
    "    #medium_classes = classes[:,1]\n",
    "    imgs = imgs.to(opt.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(imgs).last_hidden_state[:,0,:].cpu().numpy()\n",
    "    embeds.append(out)\n",
    "    labels.append(classes)\n",
    "    \n",
    "    if i > 3000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aee4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
